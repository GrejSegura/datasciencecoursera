---
title: "Machine Learning: Human Activity Recognition"
author: "Grejell Segura"
date: "9/6/2017"
output: md_document
---

# Human Activity Recognition

## Overview

This report walkthroughs a classification problem using the Weight Lifting Exercise Dataset. The goal of this project is to build a machine learning algorithm that predicts the manner of exercise which the user is performing. This will also serve as a walkthrough to how the model was built from raw data extraction to model building. A testing data set was given with 20 observation and will be used to conduct a prediction test at the end of this report. From the data source, it says:

"Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

### Loading the data and libraries

```{r message = FALSE}

library(caret)
library(randomForest)
library(RCurl)
library(lubridate)
set.seed(321)

url.train <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
url.test <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

data.train <- read.csv(text = url.train) ## training data set
data.test <- read.csv(text = url.test) ## testing data set
dim(data.train)
dim(data.test)
```
  
data.train has 19,622 observations and 160 variables. Moreover, like I said, the testing data set has 20 observations.

### Data Preprocessing

Let us check first if there are NAs or missing values in the data.
```{r message = FALSE}
any(is.na(data.train))  ## checking if NAs exist
```


The result for the command summary(data.train) was not shown here as it has a lot of information given the 160 features. Nevertheless, it was studied and the observations were noted and considered.
  
There are a number of unwanted and missing values upon checking the summary. It can be observed that the data has NAs, blanks and even the string "#DIV/0!" which is basically an error message when you calculate something in MS excel. These has to be either replaced or removed. I set the allowed number of missing values to be less than 20% of the observations for every features and try the imputation of this missing values if applicable. Those which has more than 20% of missing values will automatically be removed.
```{r message = FALSE}
naColumns <- colSums(is.na(data.train) | data.train =="" | data.train == "#DIV/0!")/nrow(data.train)
naColumns <- ifelse(naColumns > 0.2, FALSE, TRUE)

data.train <- data.train[, naColumns] ## remove the columns with more than 20% missing values
data.test <- data.test[, naColumns] ## applying the same to testing data
any(is.na(data.train))  ## checking if NAs exist
any(data.train == "")  ## checking for blanks
any(data.train == "#DIV/0!") ## checking for error string
```

The data has no more NAs and missing values. Imputation is not needed then. Let us proceed on checking the summary for the remaining features.

```{r message = FALSE}
summary(data.train)
```

There are features that are NOT needed here. Upon examining, X has to be removed as it is only an index of the observation. user_name should be removed as well along with the timestamps as it give any useful information.

```{r message = FALSE}
data.train <- data.train[, -c(1, 2, 3, 4, 5, 6, 7)]
data.test <- data.test[, -c(1, 2, 3, 4, 5, 6, 7)] ## applying the same to the testing data
```

We now have a clean data that has 53 features.
  
  
### Model Building

Let us create a data partition for training and testing. 65% of the data was used for training while the rest was alloted for out-of-sample testing.

```{r message = FALSE}
index <- createDataPartition(data.train$classe, 2, p = 0.65, list = FALSE)
train <- data.train[index, ]
test <- data.train[-index, ]
```

The algorithm of choice is the random forest which is a very powerful machine learning method. The package "randomForest" was used for this.
  
Since randomForest, for some reason needed to have the formula written on the model training function, we need to create a formula to address this.
```{r message = FALSE}
names <- names(train)[1:(length(train)-1)]
names.paste <- paste("classe", paste(names, collapse = "+"), sep = "~")
```

Now, we train the model by using all the default parameters in the function randomForest. The number of trees grown is 100 to minimize the time consumption on the training while the default number of features per tree is square root of the total number of features. The argument "importance" was also set to TRUE to check the relative importance of the features later on.
```{r message = FALSE}
rf.model <- randomForest(as.formula(names.paste), train, importance = TRUE, ntree = 100)
min(rf.model$err.rate)
```

The error rate for the in-sample-test was 0. This is pretty remarkable.

Let us proceed on testing the model to the alloted test data and see if the model over-fitted.


```{r message = FALSE}
pred.rf <- predict(rf.model, test)
confusionMatrix(pred.rf, test[, "classe"])
```

Amazingly, the model has correctly predicted all but 8 of the observations. That is at 99.66% accuracy.

Let us check the plot.
```{r message  = FALSE}
plot(rf.model)
```
  
  The plot basically tells that the out-of-bag error converged before the 100th tree.
  
Let us also check the variable importance to see which feature is relatively more important than the others.

```{r message = FALSE}
imp <- as.data.frame(rf.model$importance)
row.names(imp)[which(imp$MeanDecreaseGini == max(imp$MeanDecreaseGini))]
```
The result show that roll_belt is the most important among the features.

### Predicting the Test Data

There are 20 observations given for the testing data. It has been cleaned up to replicate the number of features in the training set.

```{r message = FALSE}
pred.rf.testing <- predict(rf.model, data.test)
pred.rf.testing
```

## Conclusion

The model build was based on the random forest algorithm. It has an amazing 99.66% accuracy on the out-of-sample testing data which is very impressive.